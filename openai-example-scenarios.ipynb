{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: I am leveraging the article [OpenAI APIs with Python — Complete Guide](https://medium.com/@marc.bolle/openai-apis-with-python-complete-guide-d933fb770f95), however, I am using the newer SDK and Rest API calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install OpenAI Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please note, you may not have to run this cell, if you already install\n",
    "%pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Authenticate by using API keys\n",
    " - Load API Key first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .env file\n",
    "\n",
    "# Import load_dotenv\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import openai\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "print(api_key)\n",
    "openai.api_key = api_key#os.environ[\"OPENAI_API_KEY\"]  # Set your OpenAI API key\n",
    "print (openai.api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Requests\n",
    "\n",
    "You can request the OpenAI API using any of the openai module’s methods. For instance, the script below lists all the **OpenAI** models using the **list()** method of the **openai.Model** class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "  # api_key=os.environ['OPENAI_API_KEY'],  # this is also the default, it can be omitted\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns a list of all OpenAI models\n",
    "models = client.models.list()\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Invoke Chat Completion API\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[ \n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"The quick brown fox\"\n",
    "      }\n",
    "  ],\n",
    "  temperature=0,\n",
    "  max_tokens=256\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New API Call\n",
    "prompt=\"write you a beautiful rhyming poem on any finding inner peace\"\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[ \n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "      }\n",
    "  ],\n",
    "  temperature=0,\n",
    "  max_tokens=256\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New API Call\n",
    "text=\"Hello, how are you?\"\n",
    "prompt = f\"Translate from English to French: {text}\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[ \n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "      }\n",
    "  ],\n",
    "  temperature=0,\n",
    "  max_tokens=256\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New API Call\n",
    "text=\"I love ice cream!\"\n",
    "prompt=f\"Sentiment analysis: {text}\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[ \n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "      }\n",
    "  ],\n",
    "  temperature=0,\n",
    "  max_tokens=256\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New API Call\n",
    "text=\"f*** y**!\"\n",
    "prompt=f\"Is this inappropriate or offensive content: {text}\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[ \n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "      }\n",
    "  ],\n",
    "  temperature=0,\n",
    "  max_tokens=256\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New API Call\n",
    "prompt=\"Create a Python function that sorts letters in a word\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[ \n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "      }\n",
    "  ],\n",
    "  temperature=0,\n",
    "  max_tokens=256\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_word(word):\n",
    "   \n",
    "    # create a list of letters\n",
    "    letters = list(word)\n",
    "    \n",
    "    # sort letters alphanumerically\n",
    "    letters.sort()\n",
    "    \n",
    "    # join list of sorted letters to form a string\n",
    "    sorted_word = ''.join(letters)\n",
    "    \n",
    "    return sorted_word\n",
    "\n",
    "# word = input(\"Enter a word: \")\n",
    "print(sort_word('happy')) # 'ahppy'\n",
    "# print(sort_word(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New API Call\n",
    "text = \"Dubai is the most populous city in the United Arab Emirates (UAE) and the capital of the Emirate of Dubai, the most populated of the 7 emirates of the United Arab Emirates.\"\n",
    "prompt=f\"Summarize this text:{text}\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[ \n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "      }\n",
    "  ],\n",
    "  temperature=0,\n",
    "  max_tokens=256\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New API Call\n",
    "# insert the beginning of the text to be completed\n",
    "prompt_text = \"\"\" List of the latest 5 Presidents of the United States:\n",
    "1. Joe Biden\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[ \n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt_text\n",
    "      }\n",
    "  ],\n",
    "  temperature=0,\n",
    "  max_tokens=256\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_text = \"\"\" Convert the following list of emotions to emojis:\n",
    "1. Happy\n",
    "2. Sad\n",
    "3. Cry\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[ \n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt_text\n",
    "      }\n",
    "  ],\n",
    "   max_tokens=100,\n",
    "  temperature = 0,\n",
    "  n = 1\n",
    ")\n",
    "\n",
    "emojis = response.choices[0].message.content.strip()\n",
    "\n",
    "print(emojis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another Chat completion example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[{\"role\": \"user\", \"content\": \"tell me a joke\"}]\n",
    ")\n",
    "\n",
    "chat_response = completion.choices[0].message.content\n",
    "print(chat_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat completion with instructions\n",
    "\n",
    "We can pass a request to the model and ask it to take on a specific role in its completion. To do that, we must define the system role.\n",
    "\n",
    "The system role, also known as the system message, is included at the beginning of the array. You can provide various information in the system role including:\n",
    "- A brief description of the assistant,\n",
    "- Personality traits of the assistant,\n",
    "- Instructions or rules you would like the assistant to follow,\n",
    "- Data or information needed for the model.\n",
    "\n",
    "The system role is optional but it’s recommended to at least include a basic one to get the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4o\",\n",
    "  messages=[{\"role\": \"system\", \"content\": \"you are Winston Churchill\"},\n",
    "            {\"role\": \"user\", \"content\": \"explain the Normandy landings in one sentence\"}]\n",
    ")\n",
    "\n",
    "chat_response = completion.choices[0].message.content\n",
    "print(chat_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat completion with few shot learning\n",
    "\n",
    "You can ask the model to complete a series of messages between the user and the assistant. This set of messages in the prompt will act as a few shot examples that can be used to seed answers to typical questions or teach the model specific behaviors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[{\"role\": \"user\", \"content\": \"Tell me a joke\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"What goes up but never ever comes down?\"},\n",
    "            {\"role\": \"user\", \"content\": \"I don't know, tell me\"}]\n",
    ")\n",
    "\n",
    "chat_response = completion.choices[0].message.content\n",
    "print(chat_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### You can also include relevant data or information in the system message to give the model extra context for the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4o\",\n",
    "  messages=[{\"role\": \"system\", \"content\": \"Annie is 35 years old\"},\n",
    "            {\"role\": \"user\", \"content\": \"How old is Annie?\"}]\n",
    ")\n",
    "\n",
    "chat_response = completion.choices[0].message.content\n",
    "print(chat_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversation loop like ChatGPT\n",
    "\n",
    "The models have no memory. That’s why in the basic examples so far shown, the assistant can’t recall the previous messages if you make a new API request.\n",
    "\n",
    "We can, however, establish a conversation loop to have a kind of “ChatGPT capability”. We do that by storing all previous queries and responses in arrays and sending them with each new query. As a result, the model retains the context of the prior queries and responses.\n",
    "\n",
    "When you run the following code, you will see a blank console window. Enter your first question in the box and press enter. After receiving an answer, you can repeat the process and continue to ask questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "\n",
    "while(True):\n",
    "    user_input = input()      \n",
    "    conversation.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\", \n",
    "        messages = conversation\n",
    "    )\n",
    "\n",
    "    conversation.append({\"role\": \"assistant\", \"content\": response.choices[0].message.content})\n",
    "    print(\"\\n\" + response.choices[0].message.content+ \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai-demo-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
